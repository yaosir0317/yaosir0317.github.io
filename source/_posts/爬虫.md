---
title: 爬虫
date: 2019-02-27 16:36:06
tags: Python
categories: Python
---

# 什么是爬虫

爬虫就是通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程。

<!--more-->

# 哪些语言可以实现爬虫

**php**：可以实现爬虫，但是php在实现爬虫中支持多线程和多进程方面做的不好。

**java**：可以实现爬虫。java可以非常好的处理和实现爬虫，是唯一可以与python并驾齐驱且是python的头号劲敌。但是java实现爬虫代码较为臃肿，重构成本较大。

**c/c++**：可以实现爬虫。但是使用这种方式实现爬虫纯粹是是某些人（大佬们）能力的体现，却不是明智和合理的选择。

**python**：可以实现爬虫。python实现和处理爬虫语法简单，代码优美，支持的模块繁多，学习成本低，具有非常强大的框架（scrapy等）！

# 爬虫的分类

## 通用爬虫

通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。  简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理(提取关键字、去掉广告)，最后提供一个用户检索接口。 

## 聚焦爬虫

聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。

# robots.txt协议

如果自己的门户网站中的指定页面中的数据不想让爬虫程序爬取到的话，那么则可以通过编写一个robots.txt的协议文件来约束爬虫程序的数据爬取。robots协议的编写格式可以观察淘宝网的robots（访问www.taobao.com/robots.txt即可）。但是需要注意的是，该协议只是相当于口头的协议，并没有使用相关技术进行强制管制(防君子不防小人).

# 反爬虫

门户网站通过相应的策略和技术手段，防止爬虫程序进行网站数据的爬取。

# 反反爬虫

爬虫程序通过相应的策略和技术手段，破解门户网站的反爬虫手段，从而爬取到相应的数据。

# requests模块

requests模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占据着重要的地位。

## 请求载体身份标识的伪装：

- User-Agent：请求载体身份标识，通过浏览器发起的请求，请求载体为浏览器，则该请求的User-Agent为浏览器的身份标识，使用爬虫程序发起的请求，则该请求的载体为爬虫程序，则该请求的User-Agent为爬虫程序的身份标识。可以通过判断该值来获知该请求的载体究竟是基于哪款浏览器还是基于爬虫程序。
- 反爬机制：某些门户网站会对访问该网站的请求中的User-Agent进行捕获和判断，如果该请求的UA为爬虫程序，则拒绝向该请求提供数据。
- 反反爬策略：将爬虫程序的UA伪装成某一款浏览器的身份标识。

## 使用

```
requests.get(url, params, headers, proxies)  # url地址/携带的参数/请求头信息/代理
requests.get()  # get请求获取
requests.post()  # post请求获取
requests.get().text  # 返回原网页内容
requests.get().content  # 返回二进制
requests.get().json()  # 返回json数据
```

# 正解解析

## 常用正则表达式

**常用元字符**

|  .   | 匹配除换行符以外的任意字符 |
| :--: | -------------------------- |
|  \w  | 匹配字母或数字或下划线     |
|  \s  | 匹配任意的空白符           |
|  \d  | 匹配数字                   |
|  \b  | 匹配单词的开始或结束       |
|  ^   | 匹配字符串的开始           |
|  $   | 匹配字符串的结束           |

**常用限定符**

|   *   | 重复零次或更多次 |
| :---: | ---------------- |
|   +   | 重复一次或更多次 |
|   ?   | 重复零次或一次   |
|  {n}  | 重复n次          |
| {n,}  | 重复n次或更多次  |
| {n,m} | 重复n到m次       |

**常用反义词**

|    \W    | 匹配任意不是字母，数字，下划线，汉字的字符 |
| :------: | ------------------------------------------ |
|    \S    | 匹配任意不是空白符的字符                   |
|    \D    | 匹配任意非数字的字符                       |
|    \B    | 匹配不是单词开头或结束的位置               |
|   [^x]   | 匹配除了x以外的任意字符                    |
| [^aeiou] | 匹配除了aeiou这几个字母以外的任意字符      |

**特殊**(包括python的re模块)

|  (ab)  | 分组                         |
| :----: | ---------------------------- |
|   .*   | 贪婪匹配                     |
|  .*?   | 惰性匹配                     |
|  re.I  | 忽略大小写                   |
|  re.M  | 多行匹配                     |
|  re.S  | 单行匹配                     |
| re.sub | 正则表达式, 替换内容, 字符串 |

# Xpath解析

## 常用xpath表达式

```python
# 属性定位： 
    '''找到class属性值为song的div标签'''
    //div[@class="song"] 
# 层级&索引定位：
    '''找到class属性值为tang的div的直系子标签ul下的第二个子标签li下的直系子标签a'''
    //div[@class="tang"]/ul/li[2]/a
# 逻辑运算：
    '''找到href属性值为空且class属性值为du的a标签'''
    //a[@href="" and @class="du"]
# 模糊匹配：
    //div[contains(@class, "ng")]
    //div[starts-with(@class, "ta")]
# 取文本：
    '''/表示获取某个标签下的文本内容
       //表示获取某个标签下的文本内容和所有子标签下的文本内容'''
    //div[@class="song"]/p[1]/text()
    //div[@class="tang"]//text()
# 取属性：
    //div[@class="tang"]//li[2]/a/@href
```

## etree

```python
from lxml import etree
# 将html文档或者xml文档转换成一个etree对象，然后调用对象中的方法查找指定的节点
# 本地文件：
	tree = etree.parse(文件名)
	tree.xpath("xpath表达式")
# 网络数据
	tree = etree.HTML(网页内容字符串)
	tree.xpath("xpath表达式")
```

### 使用

```python
# 使用xpath对url_conten进行解析
# 使用xpath解析从网络上获取的数据
tree=etree.HTML(url_content)
# 解析获取当页所有的标题
title_list=tree.xpath('xpath表达式')
```

# BeautifulSoup解析

```
from bs4 import BeautifulSoup
```

## 使用

```python
soup = BeautifulSoup('字符串类型或者字节类型', 'lxml')
（1）根据标签名查找
    - soup.a   只能找到第一个符合要求的标签
（2）获取属性
    - soup.a.attrs  获取a所有的属性和属性值，返回一个字典
    - soup.a.attrs['href']   获取href属性
    - soup.a['href']   也可简写为这种形式
（3）获取内容
    - soup.a.string
    - soup.a.text
    - soup.a.get_text()
    【注意】如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容
（4）find：找到第一个符合要求的标签
    - soup.find('a')  找到第一个符合要求的
    - soup.find('a', title="xxx")
    - soup.find('a', alt="xxx")
    - soup.find('a', class_="xxx")
    - soup.find('a', id="xxx")
（5）find_all：找到所有符合要求的标签
    - soup.find_all('a')
    - soup.find_all(['a','b']) 找到所有的a和b标签
    - soup.find_all('a', limit=2)  限制前两个
（6）根据选择器选择指定的内容
    select:soup.select('#feng')
        - 常见的选择器：标签选择器(a)、类选择器(.)、id选择器(#)、层级选择器
            - 层级选择器：
            div .dudu #lala .meme .xixi  下面好多级
            div > p > a > .lala          只能是下面一级
【注意】select选择器返回永远是列表，需要通过下标提取指定的对象
```





